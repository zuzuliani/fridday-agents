{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Working with Language Models in LangChain\n",
    "\n",
    "In this tutorial, we'll explore how to work with language models in LangChain, focusing on the Groq LLM. We'll cover connecting to the model, creating prompt templates, building chains, and handling responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to Language Models\n",
    "\n",
    "First, let's set up our environment and connect to the Groq LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I assist you today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 33, 'total_tokens': 43, 'completion_time': 0.05, 'prompt_time': 0.003967551, 'queue_time': 0.047193639, 'total_time': 0.053967551}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_92412bc7e4', 'finish_reason': 'stop', 'logprobs': None} id='run-ed95a062-2ed6-4445-a37a-6eb2fd2937a6-0' usage_metadata={'input_tokens': 33, 'output_tokens': 10, 'total_tokens': 43}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"qwen-2.5-32b\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.2, \"seed\": 1337}\n",
    "    )\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Prompt Templates\n",
    "\n",
    "Prompt templates allow us to create reusable prompts with input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question:\n",
      "You answers in the English language.\n",
      "Question: What is the capital of France?\n",
      "Answer: Let's approach this step-by-step:\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt template\n",
    "template = \"\"\"Answer the following question:\n",
    "You answers in the {language} language.\n",
    "Question: {question}\n",
    "Answer: Let's approach this step-by-step:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"language\"])\n",
    "\n",
    "# Use the prompt template\n",
    "question = \"What is the capital of France?\"\n",
    "formatted_prompt = prompt.format(question=question,language=\"English\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Simple Prompt Chains\n",
    "\n",
    "Now, let's create a chain that combines our prompt template with the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vitesse de la lumière dans le vide est d'environ 299 792 kilomètres par seconde. Cette valeur est souvent arrondie à 300 000 kilomètres par seconde pour des calculs plus simples. Cette constante est fondamentale en physique et est notée par la lettre \"c\".\n"
     ]
    }
   ],
   "source": [
    "# Create a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"question\":\"What is the speed of light?\",\"language\":\"French\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Model Responses\n",
    "\n",
    "Let's explore different ways to handle and process model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: content='Here are three prime numbers:\\n\\n1. 2\\n2. 3\\n3. 5\\n\\nThese numbers are prime because they are only divisible by 1 and themselves.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 34, 'total_tokens': 71, 'completion_time': 0.185, 'prompt_time': 0.004139689, 'queue_time': 0.04928391, 'total_time': 0.189139689}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_92412bc7e4', 'finish_reason': 'stop', 'logprobs': None} id='run-a9390958-2343-46e9-9280-93e5647e8f92-0' usage_metadata={'input_tokens': 34, 'output_tokens': 37, 'total_tokens': 71}\n",
      "\n",
      "Chain response: Bien sûr, je vais vous donner quelques noms d'arbres en français :\n",
      "\n",
      "1. Chêne\n",
      "2. Hêtre\n",
      "3. Bouleau\n",
      "4. Pin\n",
      "5. Sapin\n",
      "6. Orme\n",
      "7. Peuplier\n",
      "8. Érable\n",
      "9. Châtaignier\n",
      "10. Noisetier\n",
      "\n",
      "Ces arbres sont couramment trouvés en France et dans d'autres pays européens. Chacun a ses propres caractéristiques et utilisations.\n"
     ]
    }
   ],
   "source": [
    "# Get the raw response\n",
    "raw_response = llm.invoke(\"List three prime numbers.\")\n",
    "print(\"Raw response:\", raw_response)\n",
    "\n",
    "# Using the chain with a dictionary input\n",
    "chain_response = chain.invoke({\"question\": \"tree Names\",\"language\": \"French\"})\n",
    "print(\"\\nChain response:\", chain_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "Parsed list: ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'pink', 'purple', 'brown', 'black', 'white', 'gray', 'silver', 'gold', 'beige', 'maroon', 'olive', 'cyan', 'magenta', 'teal', 'navy', 'azure', 'lavender', 'rose', 'scarlet', 'crimson', 'mauve', 'taupe', 'chartreuse', 'periwinkle', 'coral', 'peach', 'tangerine', 'lemon', 'emerald', 'forest', 'sky', 'cerulean', 'amethyst', 'rosewood', 'ivory', 'ecru', 'flax', 'taupe', 'umber', 'brick', 'claret', 'crimson', 'ochre', 'vermilion', 'azure', 'cerulean', 'chartreuse', 'crimson', 'cyan', 'emerald', 'forest', 'indigo', 'ivory', 'azure', 'beige', 'black', 'blue', 'blue-gray', 'blue-green', 'blue-violet', 'brown', 'brown-orange', 'bronze', 'brun', 'burgundy', 'byzantium', 'cerise', 'cerulean', 'chestnut', 'chrome-yellow', 'cobalt', 'coffee', 'cool-gray', 'copper', 'copper-red', 'coral', 'coral-pink', 'coral-red', 'cream', 'cyan', 'deep-pink', 'deep-purple', 'deep-salmon', 'deep-sky-blue', 'denim', 'desert', 'desert-sand', 'dark-blue', 'dark-brown', 'dark-cyan', 'dark-gray', 'dark-green', 'dark-khaki', 'dark-magenta', 'dark-olive-green', 'dark-orange', 'dark-orange-red', 'dark-orangered', 'dark-pink', 'dark-red', 'dark-salmon', 'dark-sea-green', 'dark-slate-blue', 'dark-slate-gray', 'dark-turquoise', 'dark-violet', 'deep-pink', 'deep-purple', 'deep-salmon', 'deep-sky-blue', 'denim', 'desert', 'desert-sand', 'dodger-blue', 'dark-turquoise', 'dark-violet', 'deep-pink', 'deep-purple', 'deep-salmon', 'deep-sky-blue', 'denim', 'desert', 'desert-sand', 'dodger-blue']\n"
     ]
    }
   ],
   "source": [
    "# Parsing structured output\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"List 100 {item}. {format_instructions} write only the colors, nothing else\",\n",
    "    input_variables=[\"item\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = list_prompt | llm |output_parser\n",
    "result = chain.invoke({\"item\":\"colors\"})\n",
    "print(type(result))\n",
    "print(\"\\nParsed list:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Prompt Engineering\n",
    "\n",
    "Here are some best practices for effective prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------\n",
      "Specific Prompt Result:\n",
      "Quantum entanglement is a fascinating and somewhat mysterious phenomenon in quantum physics. To understand it, let's start with a simple analogy and then move to the actual concept.\n",
      "\n",
      "Imagine you have two dice, and you decide to play a game. You roll the dice, but there's a twist: no matter how far apart you take these dice, if one shows a 6, the other will always show a 1, and vice versa. This is a bit like entanglement, but in the quantum world, it's even more surprising because the dice (or particles) don't have to be rolled (or measured) to know what the other will show. They are connected in such a way that the state of one instantly influences the state of the other, no matter how far apart they are.\n",
      "\n",
      "In quantum physics, particles like electrons or photons can become entangled. When two particles are entangled, the quantum state of one particle is directly related to the quantum state of the other, no matter the distance between them. This means that if you measure a property (like spin or polarization) of one particle, you instantly know the corresponding property of the other particle, even if they are light-years apart.\n",
      "\n",
      "This phenomenon was so strange that even Albert Einstein, one of the greatest physicists, was skeptical of it, famously referring to it as \"spooky action at a distance.\" However, numerous experiments have confirmed that quantum entanglement is a real and fundamental aspect of our universe, challenging our classical ideas about how information can be transmitted and how particles can be connected.\n",
      "\n",
      "Entanglement is not just a theoretical curiosity; it has practical applications in technologies like quantum computing and quantum cryptography, where it can be used to perform calculations and secure communications in ways that are impossible with classical systems.\n",
      "\n",
      "---------------------------------------------------\n",
      "Few-shot Prompt Result:\n",
      "Sentiment: Neutral\n",
      "\n",
      "The text \"This movie was okay, I guess.\" indicates a neutral sentiment. The word \"okay\" suggests a middle ground, neither particularly good nor bad, and \"I guess\" adds a slight hesitation or lack of enthusiasm, which does not tilt the sentiment decidedly towards positive or negative.\n",
      "\n",
      "---------------------------------------------------\n",
      "Step-by-step Prompt Result:\n",
      "To solve the problem of calculating the area of a circle with a radius of 5 cm, follow these steps:\n",
      "\n",
      "1. **Identify the key information:**\n",
      "   - The shape is a circle.\n",
      "   - The radius (r) of the circle is 5 cm.\n",
      "\n",
      "2. **Determine the appropriate formula or method:**\n",
      "   - The formula to calculate the area (A) of a circle is \\( A = \\pi r^2 \\), where \\( r \\) is the radius of the circle.\n",
      "\n",
      "3. **Apply the formula or method step-by-step:**\n",
      "   - Substitute the value of the radius into the formula: \\( A = \\pi \\times 5^2 \\).\n",
      "   - Calculate \\( 5^2 \\), which is 25.\n",
      "   - Multiply 25 by \\( \\pi \\). The value of \\( \\pi \\) is approximately 3.14159.\n",
      "   - Therefore, \\( A = 25 \\times \\pi \\approx 25 \\times 3.14159 \\).\n",
      "\n",
      "4. **Check your answer:**\n",
      "   - Perform the multiplication: \\( 25 \\times 3.14159 = 78.53975 \\).\n",
      "   - The area of the circle is approximately 78.54 square centimeters (cm²).\n",
      "\n",
      "So, the area of the circle with a radius of 5 cm is approximately 78.54 cm².\n"
     ]
    }
   ],
   "source": [
    "# 1. Be specific and provide context\n",
    "specific_prompt = PromptTemplate(\n",
    "    template=\"You are an expert in {field}. Explain {concept} in simple terms for a beginner.\",\n",
    "    input_variables=[\"field\", \"concept\"]\n",
    ")\n",
    "\n",
    "# 2. Use examples (few-shot learning)\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"Classify the sentiment of the following text as positive, negative, or neutral.\n",
    "\n",
    "Example 1:\n",
    "Text: I love this product!\n",
    "Sentiment: Positive\n",
    "\n",
    "Example 2:\n",
    "Text: This is the worst experience ever.\n",
    "Sentiment: Negative\n",
    "\n",
    "Example 3:\n",
    "Text: The weather is cloudy today.\n",
    "Sentiment: Neutral\n",
    "\n",
    "Now, classify the following text:\n",
    "Text: {text}\n",
    "Sentiment:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# 3. Break complex tasks into steps\n",
    "step_prompt = PromptTemplate(\n",
    "    template=\"\"\"To solve the problem '{problem}', follow these steps:\n",
    "1. Identify the key information\n",
    "2. Determine the appropriate formula or method\n",
    "3. Apply the formula or method step-by-step\n",
    "4. Check your answer\n",
    "\n",
    "Now, solve the problem:\"\"\",\n",
    "    input_variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Test the prompts\n",
    "chains = {\n",
    "    \"Specific\": specific_prompt | llm,\n",
    "    \"Few-shot\": few_shot_prompt | llm,\n",
    "    \"Step-by-step\": step_prompt | llm \n",
    "    }\n",
    "\n",
    "for name, chain in chains.items():\n",
    "    print(f\"\\n---------------------------------------------------\\n{name} Prompt Result:\")\n",
    "    if name == \"Specific\":\n",
    "        print(chain.invoke({\"field\":\"physics\", \"concept\":\"quantum entanglement\"}).content)\n",
    "    elif name == \"Few-shot\":\n",
    "        print(chain.invoke({\"text\":\"This movie was okay, I guess.\"}).content)\n",
    "    else:\n",
    "        print(chain.invoke({\"problem\":\"Calculate the area of a circle with radius 5 cm\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of working with language models in LangChain, including connecting to models, creating prompt templates, building chains, handling responses, and implementing best practices for prompt engineering. These skills will serve as a foundation for building more complex applications with LangChain in future tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
