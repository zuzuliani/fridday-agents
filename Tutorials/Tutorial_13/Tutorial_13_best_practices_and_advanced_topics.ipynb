{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Best Practices and Advanced Topics\n",
    "\n",
    "In this tutorial, we'll cover best practices and advanced topics for developing and deploying LangChain and LangGraph applications. We'll explore performance optimization, handling rate limits and API costs, security considerations, deployment strategies, and monitoring and logging in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.cache import InMemoryCache\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance optimization techniques\n",
    "\n",
    "Let's explore some performance optimization techniques for LangChain applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213011/1461860608.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/tmp/ipykernel_213011/1461860608.py:19: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  return [chain.run(topic) for topic in topics]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.4 ms, sys: 7 ms, total: 94.4 ms\n",
      "Wall time: 4.38 s\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'await' outside function (<timed exec>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[2], line 34\u001b[0m\n    get_ipython().run_line_magic('time', 'async_results = await process_topics_async(topics)')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m in \u001b[1;35mrun_line_magic\u001b[0m\n    result = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/magics/execution.py:1321\u001b[0m in \u001b[1;35mtime\u001b[0m\n    code = self.shell.compile(expr_ast, source, mode)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/lib/python3.12/codeop.py:126\u001b[0;36m in \u001b[0;35m__call__\u001b[0;36m\n\u001b[0;31m    codeob = compile(source, filename, symbol, flags, True)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<timed exec>:1\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'await' outside function\n"
     ]
    }
   ],
   "source": [
    "# Enable caching\n",
    "import langchain\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# Create an async version of the LLM\n",
    "async_llm = llm.agenerate\n",
    "\n",
    "# Define a simple prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a short paragraph about {topic}.\"\n",
    ")\n",
    "\n",
    "# Create an LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Function to process topics sequentially\n",
    "def process_topics_sequential(topics: List[str]) -> List[str]:\n",
    "    return [chain.run(topic) for topic in topics]\n",
    "\n",
    "# Function to process topics asynchronously\n",
    "async def process_topics_async(topics: List[str]) -> List[str]:\n",
    "    async_chain = LLMChain(llm=async_llm, prompt=prompt)\n",
    "    tasks = [async_chain.arun(topic) for topic in topics]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Compare performance\n",
    "topics = [\"Python\", \"Machine Learning\", \"Artificial Intelligence\", \"Data Science\", \"Web Development\"]\n",
    "\n",
    "# Sequential processing\n",
    "%time sequential_results = process_topics_sequential(topics)\n",
    "\n",
    "# Asynchronous processing\n",
    "%time async_results = await process_topics_async(topics)\n",
    "\n",
    "print(f\"Sequential processing time: {time_sequential}\")\n",
    "print(f\"Asynchronous processing time: {time_async}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling rate limits and API costs\n",
    "\n",
    "Let's implement a system to handle rate limits and track API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Machine learning is a subset of artificial intelligence that involves training algorithms to learn patterns and make predictions or decisions based on data. It works by feeding large amounts of data to a computer system, which then uses statistical techniques to identify relationships and trends within the data. The algorithm can then use this learned knowledge to make predictions or classify new, unseen data. Unlike traditional programming, where a computer is explicitly told what to do, machine learning allows the system to learn and improve on its own through experience, making it a powerful tool for tasks such as image recognition, natural language processing, and predictive analytics.' response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 46, 'total_tokens': 165, 'completion_time': 0.476, 'prompt_time': 0.014790308, 'queue_time': 0.005393032000000001, 'total_time': 0.490790308}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-93926392-85e1-475a-aee8-6788dabcc61f-0'\n",
      "Response: Machine learning is a subset of artificial intelligence that involves training algorithms to learn patterns and make predictions or decisions based on data. It works by feeding large amounts of data to a computer system, which then uses statistical techniques to identify relationships and trends within the data. The algorithm can then use this learned knowledge to make predictions or classify new, unseen data. Unlike traditional programming, where a computer is explicitly told what to do, machine learning allows the system to learn and improve on its own through experience, making it a powerful tool for tasks such as image recognition, natural language processing, and predictive analytics.\n",
      "Total tokens: 0\n",
      "Total cost: $0.0000\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Note: OpenAI is used here as an example. Adjust for Groq as needed.\n",
    "# Initialize Groq LLM\n",
    "chat_model = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )\n",
    "\n",
    "def run_with_cost_tracking(prompt: str) -> Dict[str, Any]:\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chat_model([HumanMessage(content=prompt)])\n",
    "        print (response)\n",
    "    return {\n",
    "        \"response\": response.content,\n",
    "        \"total_tokens\": cb.total_tokens,\n",
    "        \"prompt_tokens\": cb.prompt_tokens,\n",
    "        \"completion_tokens\": cb.completion_tokens,\n",
    "        \"total_cost\": cb.total_cost,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = run_with_cost_tracking(\"Explain the concept of machine learning in one paragraph.\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Total tokens: {result['total_tokens']}\")\n",
    "print(f\"Total cost: ${result['total_cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Security considerations\n",
    "\n",
    "When working with LangChain and LangGraph applications, consider the following security best practices:\n",
    "\n",
    "1. Use environment variables for API keys and sensitive information\n",
    "2. Implement input validation and sanitization\n",
    "3. Use HTTPS for all API communication\n",
    "4. Implement proper authentication and authorization\n",
    "5. Regularly update dependencies\n",
    "6. Be cautious with user-provided content in prompts\n",
    "\n",
    "Here's an example of input validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid input: prompt='Tell me about AI'\n",
      "Invalid input: 1 validation error for UserInput\n",
      "prompt\n",
      "  Value error, Input contains sensitive information [type=value_error, input_value='My password is 123456', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/value_error\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    prompt: str = Field(..., min_length=1, max_length=1000)\n",
    "    \n",
    "    @field_validator('prompt')\n",
    "    def no_sensitive_info(cls, v):\n",
    "        sensitive_words = ['password', 'credit card', 'social security']\n",
    "        if any(word in v.lower() for word in sensitive_words):\n",
    "            raise ValueError(\"Input contains sensitive information\")\n",
    "        return v\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    user_input = UserInput(prompt=\"Tell me about AI\")\n",
    "    print(\"Valid input:\", user_input)\n",
    "    \n",
    "    invalid_input = UserInput(prompt=\"My password is 123456\")\n",
    "except ValueError as e:\n",
    "    print(\"Invalid input:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying LangChain and LangGraph applications\n",
    "\n",
    "Let's create a simple FastAPI application that uses our LangChain model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate_text(query: Query):\n",
    "    try:\n",
    "        result = await chain.arun(query.text)\n",
    "        return {\"generated_text\": result}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# To run the FastAPI app, use the following command in your terminal:\n",
    "# uvicorn main:app --reload\n",
    "\n",
    "# Note: This cell won't actually start the server in the notebook.\n",
    "# It's meant to be run as a separate Python file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitoring and logging in production\n",
    "\n",
    "Let's implement basic monitoring using Prometheus metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Define Prometheus metrics\n",
    "REQUESTS = Counter('api_requests_total', 'Total API requests')\n",
    "LATENCY = Histogram('api_latency_seconds', 'API latency')\n",
    "\n",
    "# Update the FastAPI endpoint to use metrics\n",
    "@app.post(\"/generate\")\n",
    "async def generate_text(query: Query):\n",
    "    REQUESTS.inc()\n",
    "    with LATENCY.time():\n",
    "        try:\n",
    "            result = await chain.arun(query.text)\n",
    "            return {\"generated_text\": result}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Add a metrics endpoint\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    from prometheus_client import generate_latest\n",
    "    return generate_latest()\n",
    "\n",
    "# Note: This cell won't actually start the server in the notebook.\n",
    "# It's meant to be run as a separate Python file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've covered several best practices and advanced topics for LangChain and LangGraph applications:\n",
    "\n",
    "1. Performance optimization techniques, including caching and asynchronous processing\n",
    "2. Handling rate limits and tracking API costs\n",
    "3. Security considerations and input validation\n",
    "4. Deploying applications using FastAPI\n",
    "5. Monitoring and logging in production using Prometheus metrics\n",
    "\n",
    "These practices will help you build more efficient, secure, and production-ready AI applications using LangChain and LangGraph.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further improve your LangChain and LangGraph applications:\n",
    "\n",
    "1. Implement more advanced caching strategies, such as using Redis for distributed caching\n",
    "2. Explore containerization (e.g., Docker) for easier deployment and scaling\n",
    "3. Implement more comprehensive logging and error handling\n",
    "4. Set up CI/CD pipelines for automated testing and deployment\n",
    "5. Explore advanced monitoring and alerting systems\n",
    "6. Consider using a reverse proxy (e.g., Nginx) for load balancing and additional security\n",
    "7. Implement rate limiting and request throttling to protect your API\n",
    "\n",
    "Remember to always follow best practices for security, performance, and scalability as you develop and deploy your AI applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
