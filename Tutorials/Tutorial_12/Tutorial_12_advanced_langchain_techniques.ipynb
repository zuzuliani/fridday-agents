{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 12: Advanced LangChain Techniques\n",
    "\n",
    "In this tutorial, we'll explore advanced LangChain techniques, including custom chain development, advanced prompt engineering, retrieval-augmented generation (RAG), and fine-tuning language models for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain, TransformChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom chain development\n",
    "\n",
    "Let's create a custom chain that processes text through multiple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world this is a test\n"
     ]
    }
   ],
   "source": [
    "class TextProcessingChain(Chain):\n",
    "    input_key: str = \"input_text\"\n",
    "    output_key: str = \"processed_text\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [self.input_key]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        text = inputs[self.input_key]\n",
    "        \n",
    "        # Step 1: Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Step 2: Remove punctuation\n",
    "        text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "        \n",
    "        # Step 3: Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return {self.output_key: text}\n",
    "\n",
    "# Use the custom chain\n",
    "text_processor = TextProcessingChain()\n",
    "result = text_processor.run(\"Hello, World! This is a   TEST.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt templating and management\n",
    "\n",
    "Let's explore advanced prompt templating techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "**Optimizing a Slow JOIN Query in PostgreSQL**\n",
      "\n",
      "To optimize a slow JOIN query in PostgreSQL, we can follow these steps:\n",
      "\n",
      "### 1. **Analyze the Query Plan**\n",
      "\n",
      "First, we need to analyze the query plan to identify the bottlenecks. You can use the `EXPLAIN` statement to get the query plan.\n",
      "\n",
      "```sql\n",
      "EXPLAIN (ANALYZE) SELECT * FROM table1\n",
      "JOIN table2 ON table1.id = table2.id\n",
      "JOIN table3 ON table2.id = table3.id\n",
      "WHERE table1.column1 = 'value1'\n",
      "AND table2.column2 = 'value2'\n",
      "AND table3.column3 = 'value3';\n",
      "```\n",
      "\n",
      "This will give you the actual query plan, including the execution time and the number of rows processed.\n",
      "\n",
      "### 2. **Indexing**\n",
      "\n",
      "Make sure that the columns used in the `JOIN` and `WHERE` clauses are indexed. Indexing can significantly speed up the query.\n",
      "\n",
      "```sql\n",
      "CREATE INDEX idx_table1_column1 ON table1 (column1);\n",
      "CREATE INDEX idx_table2_column2 ON table2 (column2);\n",
      "CREATE INDEX idx_table3_column3 ON table3 (column3);\n",
      "```\n",
      "\n",
      "### 3. **Reorder the JOINs**\n",
      "\n",
      "The order of the `JOIN`s can affect the performance of the query. Try reordering the `JOIN`s to reduce the number of rows being joined.\n",
      "\n",
      "```sql\n",
      "SELECT * FROM table1\n",
      "JOIN table3 ON table1.id = table3.id\n",
      "JOIN table2 ON table3.id = table2.id\n",
      "WHERE table1.column1 = 'value1'\n",
      "AND table2.column2 = 'value2'\n",
      "AND table3.column3 = 'value3';\n",
      "```\n",
      "\n",
      "### 4. **Use Efficient Join Types**\n",
      "\n",
      "PostgreSQL supports different join types, such as `INNER JOIN`, `LEFT JOIN`, and `FULL OUTER JOIN`. Make sure you're using the most efficient join type for your query.\n",
      "\n",
      "### 5. **Limit the Number of Rows**\n",
      "\n",
      "If you're only interested in a subset of the data, use `LIMIT` to reduce the number of rows being processed.\n",
      "\n",
      "```sql\n",
      "SELECT * FROM table1\n",
      "JOIN table2 ON table1.id = table2.id\n",
      "JOIN table3 ON table2.id = table3.id\n",
      "WHERE table1.column1 = 'value1'\n",
      "AND table2.column2 = 'value2'\n",
      "AND table3.column3 = 'value3'\n",
      "LIMIT 100;\n",
      "```\n",
      "\n",
      "### 6. **Use Efficient Aggregate Functions**\n",
      "\n",
      "If you're using aggregate functions, such as `SUM` or `COUNT`, make sure you're using the most efficient function for your query.\n",
      "\n",
      "### 7. **Regularly Vacuum and Analyze**\n",
      "\n",
      "Regularly vacuum and analyze your tables to ensure that the statistics are up to date and the query planner has the most accurate information.\n",
      "\n",
      "```sql\n",
      "VACUUM ANALYZE table1;\n",
      "VACUUM ANALYZE table2;\n",
      "VACUUM ANALYZE table3;\n",
      "```\n",
      "\n",
      "By following these steps, you should be able to optimize your slow JOIN query in PostgreSQL.\n",
      "\n",
      "**Example Use Case:**\n",
      "\n",
      "Let's say we have three tables: `orders`, `customers`, and `products`. We want to get the total revenue for each customer.\n",
      "\n",
      "```sql\n",
      "SELECT c.customer_name, SUM(o.total) AS total_revenue\n",
      "FROM orders o\n",
      "JOIN customers c ON o.customer_id = c.customer_id\n",
      "JOIN products p ON o.product_id = p.product_id\n",
      "WHERE c.country = 'USA'\n",
      "AND p.category = 'Electronics'\n",
      "GROUP BY c.customer_name;\n",
      "```\n",
      "\n",
      "To optimize this query, we can create indexes on the `customer_id` and `product_id` columns, and reorder the `JOIN`s to reduce the number of rows being joined.\n",
      "\n",
      "```sql\n",
      "CREATE INDEX idx_orders_customer_id ON orders (customer_id);\n",
      "CREATE INDEX idx_orders_product_id ON orders (product_id);\n",
      "CREATE INDEX idx_customers_country ON customers (country);\n",
      "CREATE INDEX idx_products_category ON products (category);\n",
      "\n",
      "SELECT c.customer_name, SUM(o.total) AS total_revenue\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "JOIN products p ON o.product_id = p.product_id\n",
      "WHERE c.country = 'USA'\n",
      "AND p.category = 'Electronics'\n",
      "GROUP BY c.customer_name;\n",
      "```\n",
      "\n",
      "This optimized query should run faster and more efficiently than the original query.\n"
     ]
    }
   ],
   "source": [
    "# Define a complex prompt template\n",
    "complex_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant specializing in {domain}.\"),\n",
    "    (\"human\", \"I need help with the following task: {task}\"),\n",
    "    (\"ai\", \"Certainly! I'd be happy to help you with that. Could you provide more details about {detail_request}?\"),\n",
    "    (\"human\", \"Here's more information: {additional_info}\"),\n",
    "    (\"ai\", \"Thank you for the additional information. Based on what you've told me, here's my response:\"),\n",
    "])\n",
    "\n",
    "# Create a chain using the complex prompt\n",
    "complex_chain = LLMChain(llm=llm, prompt=complex_prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = complex_chain.run({\n",
    "    \"domain\": \"software development\",\n",
    "    \"task\": \"optimizing a slow database query\",\n",
    "    \"detail_request\": \"the current query structure and database schema\",\n",
    "    \"additional_info\": \"The query is a JOIN operation across three tables with multiple WHERE clauses. The database is PostgreSQL.\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing retrieval-augmented generation (RAG)\n",
    "\n",
    "Let's implement a simple RAG system using FAISS and HuggingFace embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the document\n",
    "loader = TextLoader(\"sample_document.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"all-minilm\",base_url=os.getenv('OLLAMA_EMBEDDING_URL'))\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Define the RAG prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def retrieve_and_generate(query: str) -> str:\n",
    "    # Retrieve relevant documents\n",
    "    docs = db.similarity_search(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Generate answer\n",
    "    chain = LLMChain(llm=llm, prompt=rag_prompt)\n",
    "    return chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "# Test the RAG system\n",
    "result = retrieve_and_generate(\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning language models for specific tasks\n",
    "\n",
    "While we can't directly fine-tune the Groq model, we can simulate fine-tuning by creating a specialized prompt that adapts the model's behavior for a specific task. Let's create a 'fine-tuned' model for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product! It's amazing and works perfectly.\n",
      "Sentiment: Sentiment: Positive\n",
      "\n",
      "Explanation: The text expresses a strong positive sentiment, as indicated by the use of enthusiastic language such as \"I love\", \"amazing\", and \"works perfectly\". The tone is energetic and affirmative, conveying a high level of satisfaction with the product.\n",
      "\n",
      "Text: This is the worst experience I've ever had. Terrible customer service.\n",
      "Sentiment: Sentiment: Negative\n",
      "\n",
      "Explanation: The text expresses strong dissatisfaction with the experience and specifically mentions \"terrible customer service\". The use of words like \"worst\" and \"terrible\" convey a strong negative sentiment, indicating that the speaker is extremely unhappy with the service they received.\n",
      "\n",
      "Text: The weather is quite nice today, not too hot or cold.\n",
      "Sentiment: Sentiment: Neutral\n",
      "\n",
      "Explanation: The text describes the weather as \"quite nice,\" which has a slightly positive connotation. However, it is also described in a relatively neutral manner, focusing on the absence of extreme temperatures (\"not too hot or cold\"). The tone is calm and matter-of-fact, without any strong emotions or enthusiasm, which is why I classify the sentiment as neutral rather than positive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a sentiment analysis expert. Your task is to analyze the sentiment of the given text and classify it as positive, negative, or neutral. \n",
    "    Provide a brief explanation for your classification.\n",
    "\n",
    "    Text: {input_text}\n",
    "\n",
    "    Sentiment: \"\"\",\n",
    "    input_variables=[\"input_text\"]\n",
    ")\n",
    "\n",
    "sentiment_chain = LLMChain(llm=llm, prompt=sentiment_prompt)\n",
    "\n",
    "def analyze_sentiment(text: str) -> str:\n",
    "    return sentiment_chain.run(text)\n",
    "\n",
    "# Test the 'fine-tuned' sentiment analysis model\n",
    "texts = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",\n",
    "    \"This is the worst experience I've ever had. Terrible customer service.\",\n",
    "    \"The weather is quite nice today, not too hot or cold.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {analyze_sentiment(text)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored advanced LangChain techniques, including:\n",
    "\n",
    "1. Custom chain development for specialized text processing\n",
    "2. Advanced prompt templating and management for complex interactions\n",
    "3. Implementing a retrieval-augmented generation (RAG) system\n",
    "4. Simulating fine-tuning for specific tasks using specialized prompts\n",
    "\n",
    "These techniques allow you to create more sophisticated and tailored AI applications using LangChain and LangGraph.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further advance your skills with LangChain and LangGraph, consider:\n",
    "\n",
    "1. Experimenting with more complex custom chains and combining them with LangGraph flows\n",
    "2. Developing a prompt management system for large-scale applications\n",
    "3. Exploring advanced RAG techniques, such as hypothetical document embeddings or multi-query retrieval\n",
    "4. Investigating ways to evaluate and improve the performance of your 'fine-tuned' models\n",
    "5. Integrating these advanced techniques into a full-fledged application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
