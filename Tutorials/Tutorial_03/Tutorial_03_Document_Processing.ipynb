{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Document Processing with LangChain\n",
    "\n",
    "In this tutorial, we'll explore document processing techniques using LangChain. We'll cover loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS,Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "os.environ['HF_TOKEN']=os.getenv('HF_TOKEN')\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm =  ChatGroq(\n",
    "        model_name=\"qwen-2.5-32b\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of sample1.txt:\n",
      "# Comprehensive Overview of Artificial Intelligence\n",
      "\n",
      "## Table of Contents\n",
      "1. [Introduction to Artificial Intelligence](#introduction-to-artificial-intelligence)\n",
      "2. [History of AI](#history-of-ai)\n",
      "3. [...\n",
      "\n",
      "Number of documents loaded: 1\n",
      "Document 1 preview: # Comprehensive Overview of Artificial Intelligenc...\n"
     ]
    }
   ],
   "source": [
    "# Load a single document\n",
    "loader = TextLoader(\"sample_documents/sample1.txt\")\n",
    "document = loader.load()\n",
    "\n",
    "print(f\"Content of sample1.txt:\\n{document[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# Load multiple documents from a directory\n",
    "dir_loader = DirectoryLoader(\"sample_documents/\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1} preview: {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Carica il PDF\n",
    "loader = PyPDFLoader(\"sample_documents/sample2.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Splitting and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 52\n",
      "First split preview:\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of splits: {len(splits)}\")\n",
    "print(f\"First split preview:\\n{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Question-Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of these documents?\n",
      "Answer: The provided excerpts discuss various aspects of computational linguistics and natural language processing, including the development of annotated corpora, parsing models, attention models for NLP, and a deep learning model for abstractive summarization. The main focus seems to be on the training of a neural machine translation model, detailing the training data, batching strategy, hardware setup, and architectural variations of a Transformer model for translation tasks.\n",
      "\n",
      "Sources:\n",
      "Document 1: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "c...\n",
      "Document 2: and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for o...\n",
      "Document 3: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b...\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(splits, embedding_model)\n",
    "\n",
    "# Create a retrieval-based QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is the main topic of these documents?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result['result']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"Document {i+1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Discuss the importance of AI\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "Result 1:\n",
      "process-and outcome-based feedback. Neural Information Processing Systems (NeurIPS\n",
      "2022) Workshop on MATH-AI, 2022.\n",
      "Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D\n",
      "Goodm...\n",
      "\n",
      "Result 2:\n",
      "Quiet-STaR: Language Models Can Teach Themselves to\n",
      "Think Before Speaking\n",
      "Eric Zelikman\n",
      "Stanford University\n",
      "Georges Harik\n",
      "Notbad AI Inc\n",
      "Yijia Shao\n",
      "Stanford University\n",
      "Varuna Jayasiri\n",
      "Notbad AI Inc\n",
      "Nic...\n",
      "\n",
      "Result 3:\n",
      "Proving. CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393. eprint:\n",
      "2009.03393.\n",
      "Ben Prystawski, Michael Li, and Noah Goodman. Why think step by step? reasoning\n",
      "emerges from the locality...\n",
      "\n",
      "Question: What are some advantages of ai models?\n",
      "Answer: content='Based on the provided context, some advantages of AI models include:\\n\\n1. Ability to learn and infer unstated rationales in arbitrary text: This allows language models to think and reason implicitly, similar to humans, rather than just answering questions or completing tasks.\\n\\n2. Self-learning and improvement: The Self-Taught Reasoner (STaR) model can learn from few-shot examples and improve its reasoning abilities by inferring rationales that lead to correct answers.\\n\\n3. Ability to think before speaking: The Quiet-STaR model can teach itself to think before generating text, allowing for more thoughtful and reasoned responses.\\n\\n4. Emergence of step-by-step reasoning: AI models can learn to reason in a step-by-step manner, similar to humans, through the locality of experience.\\n\\n5. Potential for improved performance: By learning to reason and think implicitly, AI models may be able to improve their performance on a wide range of tasks, from question-answering to conversation and writing.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 199, 'prompt_tokens': 511, 'total_tokens': 710, 'completion_time': 0.796, 'prompt_time': 0.108581497, 'queue_time': 0.003413243999999996, 'total_time': 0.904581497}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None} id='run-d87968a7-8385-406d-b99b-72c3edb122b7-0' usage_metadata={'input_tokens': 511, 'output_tokens': 199, 'total_tokens': 710}\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search\n",
    "query = \"Discuss the importance of Attention\"\n",
    "search_results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Search query: {query}\\n\")\n",
    "print(\"Top 3 relevant chunks:\")\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"Result {i+1}:\\n{doc.page_content[:200]}...\\n\")\n",
    "\n",
    "# Use the search results to answer a question\n",
    "question = \"What are some Applications of Attention in our Model?\"\n",
    "context = \"\\n\".join([doc.page_content for doc in search_results])\n",
    "\n",
    "prompt = f\"Based on the following context, answer the question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of document processing with LangChain, including loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search. These techniques form the foundation for more advanced document analysis and information retrieval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
