{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Introduction to LangChain\n",
    "\n",
    "Welcome to our first tutorial on LangChain! In this notebook, we'll cover the basics of LangChain and create a simple application using the Groq LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is LangChain?\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It provides a set of tools and abstractions that make it easier to build complex LLM-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation and Setup\n",
    "\n",
    "First, let's make sure we have the necessary packages installed:\n",
    "<br> using the command pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the required modules and set up our Groq API key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature and Top_p Sampling\n",
    "\n",
    "1. Temperature:\n",
    "    - Controls the randomness/creativity of text generation\n",
    "    - Range: 0.0 to 1.0 (can go higher, but not recommended)\n",
    "    - Lower values (e.g., 0.2): More focused, deterministic output\n",
    "    - Higher values (e.g., 0.7): More diverse, creative output\n",
    "    - Temperature 0: Always selects the most probable next token\n",
    "    - Affects the probability distribution of possible tokens\n",
    "\n",
    "2. Top_p Sampling (Nucleus Sampling):\n",
    "    - Alternative to temperature sampling\n",
    "    - Range: 0.0 to 1.0\n",
    "    - Considers only the subset of tokens whose cumulative probability mass reaches the top_p threshold\n",
    "    - Example: top_p 0.1 considers only the top 10% most likely tokens\n",
    "    - Allows for dynamic vocabulary selection based on context\n",
    "\n",
    "3. Usage:\n",
    "    - Can be used independently or together\n",
    "    - Adjust based on desired level of creativity and control\n",
    "\n",
    "4. Extended Concepts:\n",
    "    - Perplexity: Measure of how well the model predicts the text. Lower perplexity indicates better prediction.\n",
    "    - Beam Search: Technique to find the most likely sequence of tokens by exploring multiple possibilities simultaneously.\n",
    "    - Length Penalty: Encourages or discourages the model to generate longer or shorter sequences.\n",
    "\n",
    "5. Best Practices:\n",
    "    - Experiment with different values for your specific use case\n",
    "    - Start with middle values (e.g., temperature 0.5, top_p 0.5) and adjust as needed\n",
    "    - For factual or technical tasks, use lower temperature and top_p\n",
    "    - For creative tasks, use higher temperature and top_p\n",
    "    - Consider using top_p sampling for more consistent results across different prompts\n",
    "\n",
    "6. Example Use Cases and Recommended Values:\n",
    "    Use Case                 | Temperature | Top_p | Description\n",
    "    ------------------------ | ----------- | ----- | -----------\n",
    "    Code Generation          | 0.2         | 0.1   | Focused, convention-adhering code\n",
    "    Creative Writing         | 0.7         | 0.8   | Diverse, exploratory text\n",
    "    Chatbot Responses        | 0.5         | 0.5   | Balanced coherence and diversity\n",
    "    Code Comment Generation  | 0.3         | 0.2   | Concise, relevant comments\n",
    "    Data Analysis Scripting  | 0.2         | 0.1   | Correct, efficient scripts\n",
    "    Exploratory Code Writing | 0.6         | 0.7   | Creative, alternative solutions\n",
    "\n",
    "7. Advanced Techniques:\n",
    "    - Fine-tuning: Adapt the model to specific tasks or domains for better performance\n",
    "    - Prompt Engineering: Craft effective prompts to guide the model's output\n",
    "    - Few-shot Learning: Provide examples in the prompt to steer the model's behavior\n",
    "\n",
    "8. Ethical Considerations:\n",
    "    - Be aware of potential biases in the model's output\n",
    "    - Implement content filtering for sensitive applications\n",
    "    - Respect copyright and intellectual property rights\n",
    "\n",
    "9. Performance Optimization:\n",
    "    - Use caching mechanisms to store frequently requested outputs\n",
    "    - Implement request batching for multiple related queries\n",
    "    - Consider using smaller models for faster response times in less complex tasks\n",
    "\n",
    "Remember to refer to the official OpenAI documentation for the most up-to-date information and best practices when working with the GPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"qwen-2.5-32b\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.5, \"seed\": 1337}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Concepts: Chains, Agents, and Memory\n",
    "\n",
    "LangChain introduces several key concepts:\n",
    "\n",
    "- **Chains**: Sequences of operations to be performed with an LLM\n",
    "- **Agents**: Autonomous entities that can use tools and make decisions\n",
    "- **Memory**: Systems for storing and retrieving information across interactions\n",
    "\n",
    "In this tutorial, we'll focus on creating a simple chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your First LangChain Application\n",
    "\n",
    "Let's create a simple application that generates a short story based on a given prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Simple LangChain LLM Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI, I don't have feelings, but I'm here and ready to help you with any questions or tasks you might have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "#Utilize the LLMChain to generate a simple response\n",
    "resp=llm.invoke(\"Hello, how are you?\")\n",
    "#Print only the text of the response\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ella stepped back into 1920s New York, her heart pounding with excitement. She meant only to observe, but a misplaced word at a speakeasy sparked a chain reaction. The jazz legend she admired never picked up his trumpet again, instead becoming a silent film star. Ella watched, horrified, as the timeline shifted, erasing the music that defined an era. Desperate to fix her mistake, she reached for her device, but the screen flickered with static. She was trapped in a world without the rhythm that once filled the night air.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 64, 'total_tokens': 180, 'completion_time': 0.58, 'prompt_time': 0.005552307, 'queue_time': 0.048915163, 'total_time': 0.585552307}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_92412bc7e4', 'finish_reason': 'stop', 'logprobs': None} id='run-7c01bcf2-8c5c-45d8-b4c0-a3e356255c20-0' usage_metadata={'input_tokens': 64, 'output_tokens': 116, 'total_tokens': 180}\n",
      "------------------------------------------------------\n",
      "Ella stepped back into 1920s New York, her heart pounding with excitement. She meant only to observe, but a misplaced word at a speakeasy sparked a chain reaction. The jazz legend she admired never picked up his trumpet again, instead becoming a silent film star. Ella watched, horrified, as the timeline shifted, erasing the music that defined an era. Desperate to fix her mistake, she reached for her device, but the screen flickered with static. She was trapped in a world without the rhythm that once filled the night air.\n",
      "Total Tokens: 180\n",
      "Input Tokens: 64\n",
      "Output Tokens: 116\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template\n",
    "template = \"\"\"\n",
    "You are a creative writer. Write a short story (about 100 words) based on the following prompt:\n",
    "\n",
    "Prompt: {prompt}\n",
    "\n",
    "Story:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])\n",
    "\n",
    "# Create a chain\n",
    "story_chain = prompt | llm\n",
    "\n",
    "# Generate a story\n",
    "story_prompt = \"A time traveler accidentally changes history\"\n",
    "result = story_chain.invoke(story_prompt)\n",
    "\n",
    "#All the text of the response\n",
    "print(result)\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "#Only the text of the response\n",
    "print(result.content)\n",
    "\n",
    "#Token Used\n",
    "print(\"Total Tokens: \"+ result.response_metadata[\"token_usage\"][\"total_tokens\"].__str__())\n",
    "print(\"Input Tokens: \"+result.response_metadata[\"token_usage\"][\"prompt_tokens\"].__str__())\n",
    "print(\"Output Tokens: \"+ result.response_metadata[\"token_usage\"][\"completion_tokens\"].__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've just created your first LangChain application. This simple example demonstrates how to use a prompt template, create a chain, and generate content using an LLM.\n",
    "\n",
    "In the next tutorial, we'll explore more advanced features of LangChain and dive deeper into working with language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
